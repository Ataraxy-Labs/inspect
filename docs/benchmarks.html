<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Benchmarks | inspect</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="container">
    <nav>
      <a class="logo" href="index.html">inspect</a>
      <div class="links">
        <a href="index.html">Home</a>
        <a href="benchmarks.html" class="active">Benchmarks</a>
        <a href="docs.html">Docs</a>
        <a href="changelog.html">Changelog</a>
        <a href="https://github.com/Ataraxy-Labs/inspect">GitHub</a>
        <a href="llms.txt">llms.txt</a>
      </div>
    </nav>

    <div style="padding: 48px 0 12px;">
      <h1 style="font-size: 28px; font-weight: 700; color: var(--accent); letter-spacing: -1px; margin-bottom: 12px;">Benchmarks</h1>
      <p style="font-size: 14px; color: var(--dim); line-height: 1.7; max-width: 600px;">
        inspect + LLM evaluated on the same benchmarks used to measure frontier code review tools. Entity-level triage focuses the LLM on the code that matters.
      </p>
    </div>

    <!-- AACR-Bench -->
    <section>
      <h2>AACR-Bench (Alibaba)</h2>
      <p class="section-desc">
        <a href="https://arxiv.org/abs/2601.19494" style="color:var(--cyan)">AACR-Bench</a> is the benchmark used to evaluate GPT-5.2, Claude 4.5 Sonnet, and other frontier LLMs for automated code review. 158 PRs, 1,169 ground truth issues from human reviewers, 50 open-source repos, 10 languages.
      </p>

      <h3 style="font-size: 15px; color: var(--accent); margin: 16px 0 16px;">Head-to-head: inspect vs Greptile vs CodeRabbit</h3>
      <p style="font-size: 13px; color: var(--dim); margin-bottom: 16px; line-height: 1.7;">
        We ran all tools on the same 20 AACR-Bench PRs (166 golden comments, 9 languages, 20 repos). Greptile via their production API. CodeRabbit via their CLI. inspect + LLM: triage with inspect, then review top entities per PR with an LLM. Same keyword-matching judge for all tools.
      </p>

      <div class="stat-cards">
        <div class="stat-card" style="border-color: var(--green);">
          <div class="stat-value" style="color: var(--green);">45.8%</div>
          <div class="stat-label">recall (inspect + GPT-5.2)</div>
          <div class="stat-detail">2x Greptile, 3.4x CodeRabbit</div>
        </div>
        <div class="stat-card" style="border-color: var(--cyan);">
          <div class="stat-value" style="color: var(--cyan);">29.2%</div>
          <div class="stat-label">F1 score</div>
          <div class="stat-detail">highest of all three tools</div>
        </div>
        <div class="stat-card" style="border-color: var(--purple);">
          <div class="stat-value" style="color: var(--purple);">355</div>
          <div class="stat-label">findings generated</div>
          <div class="stat-detail">vs 180 Greptile, 19 CodeRabbit</div>
        </div>
      </div>

      <div class="comparison-table">
        <table>
          <thead>
            <tr>
              <th>Tool</th>
              <th>Recall</th>
              <th>Precision</th>
              <th>F1</th>
              <th>Findings</th>
              <th>Speed</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong style="color:var(--accent)">inspect + GPT-5.2</strong></td>
              <td class="win">45.8%</td>
              <td>21.4%</td>
              <td class="win">29.2%</td>
              <td>355</td>
              <td class="win">5-15s/PR</td>
            </tr>
            <tr>
              <td>Greptile (API)</td>
              <td>23.5%</td>
              <td>21.7%</td>
              <td>22.5%</td>
              <td>180</td>
              <td>10-60s/PR</td>
            </tr>
            <tr>
              <td>CodeRabbit (CLI)</td>
              <td>13.3%</td>
              <td>115.8%*</td>
              <td>23.8%</td>
              <td>19</td>
              <td>2-5min/PR</td>
            </tr>
          </tbody>
        </table>
        <p style="font-size:11px;color:var(--dim2);margin-top:6px;">*CodeRabbit's precision exceeds 100% because multiple golden comments matched the same finding (19 findings caught 22 issues).</p>
      </div>

      <div class="bench-group" style="margin-top: 24px;">
        <h3>Recall comparison</h3>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name"><strong style="color:var(--accent)">inspect + GPT-5.2</strong></span>
            <span class="value" style="color:var(--green)">45.8%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar inspect-bar" data-width="45.8">45.8%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">Greptile (API)</span>
            <span class="value">23.5%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar other-bar" data-width="23.5">23.5%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">CodeRabbit (CLI)</span>
            <span class="value">13.3%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar dim-bar" data-width="13.3">13.3%</div>
          </div>
        </div>
        <div class="bench-note">20 PRs, 166 golden comments, same judge</div>
      </div>

      <div class="bench-group" style="margin-top: 24px;">
        <h3>F1 score comparison</h3>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name"><strong style="color:var(--accent)">inspect + GPT-5.2</strong></span>
            <span class="value" style="color:var(--green)">29.2%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar inspect-bar" data-width="29.2">29.2%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">CodeRabbit (CLI)</span>
            <span class="value">23.8%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar other-bar" data-width="23.8">23.8%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">Greptile (API)</span>
            <span class="value">22.5%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar other-bar" data-width="22.5">22.5%</div>
          </div>
        </div>
        <div class="bench-note">F1 = harmonic mean of recall and precision</div>
      </div>

      <p style="font-size:13px;color:var(--dim);margin-top:12px;line-height:1.7;">
        <strong style="color:var(--fg)">2x Greptile's recall, 3.4x CodeRabbit's recall. Highest F1 of all three.</strong> inspect + GPT-5.2 catches 45.8% of real issues vs Greptile's 23.5% and CodeRabbit's 13.3%, with similar precision to Greptile (21.4% vs 21.7%). Per-language: C 100%, C# 100%, Java 100%, Go 57%, Python 53%, JavaScript 54%, TypeScript 40%.
      </p>

      <p style="font-size:12px;color:var(--dim2);margin-top:8px;line-height:1.7;">
        20 diverse PRs from AACR-Bench (round-robin across repos). Recall = file + line or file + identifier match against golden comments. Precision = matches / total findings generated. All tools judged identically with keyword-matching heuristic. Greptile repos indexed on their default branch. CodeRabbit run locally with rate limit retries (free tier, ~2 reviews per 5 minutes). inspect + LLM reviews top 30 entities per PR (High/Critical/Medium first, then Low-risk for uncovered files) + 5-file gap review for diff coverage. 10 concurrent LLM calls. Entity-level dedup (20-line window + identifier overlap) reduces noise. LLM non-determinism causes ~5% variance between runs.
      </p>
    </section>

    <!-- Greptile benchmark -->
    <section>
      <h2>Greptile benchmark</h2>
      <p class="section-desc">
        <a href="https://www.greptile.com/benchmarks" style="color:var(--cyan)">Greptile's benchmark</a>: 50 PRs with planted bugs across 5 repos (Sentry, Cal.com, Grafana, Keycloak, Discourse). Ground truth from <a href="https://github.com/ai-code-review-evaluations/golden_comments" style="color:var(--cyan)">Augment's expanded golden comments</a> (141 golden comments, 51 High/Critical). Recall = binary caught/not-caught, matching Greptile's methodology. Borderline cases evaluated by LLM judge.
      </p>

      <div class="stat-cards">
        <div class="stat-card" style="border-color: var(--green);">
          <div class="stat-value" style="color: var(--green);">96.5%</div>
          <div class="stat-label">recall (inspect + Sonnet 4.6)</div>
          <div class="stat-detail">136/141 bugs caught</div>
        </div>
        <div class="stat-card" style="border-color: var(--cyan);">
          <div class="stat-value" style="color: var(--cyan);">100%</div>
          <div class="stat-label">High/Critical recall</div>
          <div class="stat-detail">51/51 bugs caught</div>
        </div>
      </div>

      <h3 style="font-size: 15px; color: var(--accent); margin-bottom: 16px;">Per-repo results</h3>
      <div class="comparison-table">
        <table>
          <thead>
            <tr>
              <th>Repo</th>
              <th>Language</th>
              <th>n</th>
              <th>inspect + Sonnet 4.6</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Cal.com</td>
              <td>TypeScript</td>
              <td>31</td>
              <td class="win">100%</td>
            </tr>
            <tr>
              <td>Keycloak</td>
              <td>Java</td>
              <td>26</td>
              <td class="win">100%</td>
            </tr>
            <tr>
              <td>Grafana</td>
              <td>Go</td>
              <td>22</td>
              <td class="win">100%</td>
            </tr>
            <tr>
              <td>Discourse</td>
              <td>Ruby</td>
              <td>28</td>
              <td class="win">96.4%</td>
            </tr>
            <tr>
              <td>Sentry</td>
              <td>Python</td>
              <td>34</td>
              <td class="win">88.2%</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3 style="font-size: 15px; color: var(--accent); margin: 24px 0 16px;">Comparison with other tools</h3>
      <p style="font-size: 13px; color: var(--dim); margin-bottom: 16px; line-height: 1.7;">
        Same benchmark, same 50 PRs, same golden comments. Other tools' recall numbers from <a href="https://www.augmentcode.com/blog/we-benchmarked-7-ai-code-review-tools-on-real-world-prs-here-are-the-results" style="color:var(--cyan)">Augment's independent evaluation</a>. inspect recall from our own evaluation using keyword-matching + LLM judge on 141 golden comments. Recall only (precision/F1 not comparable across different finding architectures).
      </p>

      <div class="comparison-table">
        <table>
          <thead>
            <tr>
              <th>Tool</th>
              <th>Recall</th>
              <th>Evaluated by</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong style="color:var(--accent)">inspect + Sonnet 4.6</strong></td>
              <td class="win">96.5%</td>
              <td>us (keyword + LLM judge)</td>
            </tr>
            <tr>
              <td>Augment Code Review</td>
              <td>55%</td>
              <td>Augment</td>
            </tr>
            <tr>
              <td>Claude Code</td>
              <td>51%</td>
              <td>Augment</td>
            </tr>
            <tr>
              <td>Greptile</td>
              <td>45%</td>
              <td>Augment</td>
            </tr>
            <tr>
              <td>CodeRabbit</td>
              <td>43%</td>
              <td>Augment</td>
            </tr>
            <tr>
              <td>Cursor (Bugbot)</td>
              <td>41%</td>
              <td>Augment</td>
            </tr>
            <tr>
              <td>GitHub Copilot</td>
              <td>34%</td>
              <td>Augment</td>
            </tr>
            <tr>
              <td>Codex Code Review</td>
              <td>29%</td>
              <td>Augment</td>
            </tr>
          </tbody>
        </table>
      </div>

      <p style="font-size:13px;color:var(--dim);margin-top:20px;line-height:1.7;">
        <strong style="color:var(--fg)">96.5% recall. 100% on High/Critical. Zero critical bugs missed.</strong> inspect + Sonnet 4.6 catches nearly every planted bug. The triage step narrows 100+ entities to ~30, so the LLM reviews focused code with full context instead of scanning entire diffs.
      </p>

      <div class="bench-group" style="margin-top: 32px;">
        <h3>Recall comparison (visual)</h3>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name"><strong style="color:var(--accent)">inspect + Sonnet 4.6</strong></span>
            <span class="value" style="color:var(--green)">96.5%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar inspect-bar" data-width="96.5">96.5%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">Augment Code</span>
            <span class="value">55%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar other-bar" data-width="55">55%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">Claude Code</span>
            <span class="value">51%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar dim-bar" data-width="51">51%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">Greptile</span>
            <span class="value">45%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar dim-bar" data-width="45">45%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">CodeRabbit</span>
            <span class="value">43%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar dim-bar" data-width="43">43%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">Cursor (Bugbot)</span>
            <span class="value">41%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar dim-bar" data-width="41">41%</div>
          </div>
        </div>
        <div class="bench-row">
          <div class="bench-label">
            <span class="name">GitHub Copilot</span>
            <span class="value">34%</span>
          </div>
          <div class="bench-bar-track">
            <div class="bench-bar dim-bar" data-width="34">34%</div>
          </div>
        </div>
        <div class="bench-note">50 PRs, 141 golden comments, same benchmark, different evaluators</div>
      </div>

      <p style="font-size:12px;color:var(--dim2);margin-top:8px;line-height:1.7;">
        Other tools' recall from <a href="https://www.augmentcode.com/blog/we-benchmarked-7-ai-code-review-tools-on-real-world-prs-here-are-the-results" style="color:var(--cyan)">Augment's blog post</a> (their own LLM-based evaluation). inspect recall from our keyword-matching heuristic + LLM judge for borderline cases (binary caught/not-caught, matching <a href="https://www.greptile.com/benchmarks" style="color:var(--cyan)">Greptile's methodology</a>). Different evaluation methods on the same benchmark. Full data on <a href="https://huggingface.co/datasets/rs545837/inspect-greptile-bench" style="color:var(--cyan)">HuggingFace</a>.
      </p>
    </section>

    <!-- How it works -->
    <section>
      <h2>How it works</h2>
      <p class="section-desc">
        inspect runs entity-level triage locally (free, &lt;1s), then sends the top entities to an LLM for focused review. The LLM sees entity-scoped code with before/after context, not a raw diff.
      </p>

      <h3 style="font-size: 15px; color: var(--accent); margin: 16px 0 16px;">Per-severity breakdown (Greptile benchmark)</h3>
      <div class="comparison-table">
        <table>
          <thead>
            <tr>
              <th>Severity</th>
              <th>n</th>
              <th>Recall</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Critical</td>
              <td>9</td>
              <td class="win">100%</td>
            </tr>
            <tr>
              <td>High</td>
              <td>42</td>
              <td class="win">100%</td>
            </tr>
            <tr>
              <td>Medium</td>
              <td>49</td>
              <td class="win">93.9%</td>
            </tr>
            <tr>
              <td>Low</td>
              <td>41</td>
              <td class="win">95.1%</td>
            </tr>
          </tbody>
        </table>
      </div>

      <p style="font-size:13px;color:var(--dim);margin-top:20px;line-height:1.7;">
        <strong style="color:var(--fg)">The pipeline.</strong> Run inspect first (free, &lt;1s) to get entity risk rankings. Send top 30 entities to an LLM with before/after code + file diff. Gap-review uncovered files. Dedup and filter by confidence. The triage step means the LLM sees focused code, not an entire diff.
      </p>
      <p style="font-size:12px;color:var(--dim2);margin-top:8px;line-height:1.7;">
        50 PRs across 5 repos. Claude Sonnet 4.6 with 10 concurrent API calls. ~30 entities per PR + 5-file gap review. Recall judged with keyword heuristic + manual overrides (binary caught/not-caught, matching <a href="https://www.greptile.com/benchmarks" style="color:var(--cyan)">Greptile's methodology</a>). Full data on <a href="https://huggingface.co/datasets/rs545837/inspect-greptile-bench" style="color:var(--cyan)">HuggingFace</a>.
      </p>
    </section>

    <!-- Speed -->
    <section>
      <h2>Speed</h2>
      <p class="section-desc">Entity extraction, dependency graph, change classification, risk scoring, commit untangling. All local, no API calls.</p>

      <h3 style="font-size: 15px; color: var(--accent); margin-bottom: 16px;">Single commit review</h3>
      <p style="font-size: 13px; color: var(--dim); margin-bottom: 16px; line-height: 1.7;">
        Time to run <code style="background:var(--surface);padding:2px 6px;border-radius:3px;font-size:12px;color:var(--cyan)">inspect diff HEAD~1</code> on a real commit. 30 runs via hyperfine, warm cache.
      </p>

      <div class="comparison-table">
        <table>
          <thead>
            <tr>
              <th>Repo</th>
              <th>Size</th>
              <th>Time</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>sem</td>
              <td>25 files, 65 entities changed</td>
              <td class="win">6ms</td>
            </tr>
            <tr>
              <td>weave</td>
              <td>80 files, 89 entities changed</td>
              <td class="win">6ms</td>
            </tr>
            <tr>
              <td>inspect</td>
              <td>50 files, large commit</td>
              <td class="win">67ms</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3 style="font-size: 15px; color: var(--accent); margin: 24px 0 16px;">Full repo history (inspect bench)</h3>
      <p style="font-size: 13px; color: var(--dim); margin-bottom: 16px; line-height: 1.7;">
        Time to analyze every commit in a repo's history: extract entities, build graph, classify changes, score risk, untangle.
      </p>

      <div class="comparison-table">
        <table>
          <thead>
            <tr>
              <th>Repo</th>
              <th>Commits</th>
              <th>Entities</th>
              <th>Wall time</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>sem</td>
              <td>38</td>
              <td>5,216</td>
              <td class="win">0.57s</td>
            </tr>
            <tr>
              <td>weave</td>
              <td>45</td>
              <td>2,854</td>
              <td class="win">1.33s</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="bench-group" style="margin-top: 32px;">
        <h3>Single commit review time (visual)</h3>
        <div class="bench-row">
          <div class="bench-label"><span class="name">sem (25 files)</span><span class="value" style="color:var(--green)">6ms</span></div>
          <div class="bench-bar-track"><div class="bench-bar inspect-bar" data-width="9">6ms</div></div>
        </div>
        <div class="bench-row">
          <div class="bench-label"><span class="name">weave (80 files)</span><span class="value" style="color:var(--green)">6ms</span></div>
          <div class="bench-bar-track"><div class="bench-bar inspect-bar" data-width="9">6ms</div></div>
        </div>
        <div class="bench-row">
          <div class="bench-label"><span class="name">inspect (50 files)</span><span class="value" style="color:var(--green)">67ms</span></div>
          <div class="bench-bar-track"><div class="bench-bar inspect-bar" data-width="100">67ms</div></div>
        </div>
        <div class="bench-note">30 runs via hyperfine -N, warm cache</div>
      </div>

      <p style="font-size:13px;color:var(--dim);margin-top:20px;line-height:1.7;">
        Powered by <a href="https://ataraxy-labs.com/sem" style="color:var(--cyan)">sem-core</a> v0.3.0: xxHash64 structural hashing, parallel tree-sitter parsing via rayon, cached git tree resolution, LTO-optimized release builds.
      </p>
    </section>

    <footer>
      <p>Built by <a href="https://ataraxy-labs.com">Ataraxy Labs</a></p>
    </footer>
  </div>
  <script>
    // Animate bars when they scroll into view
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          const bars = entry.target.querySelectorAll('.bench-bar[data-width]');
          bars.forEach((bar, i) => {
            setTimeout(() => {
              bar.style.width = bar.dataset.width + '%';
            }, i * 80);
          });
          observer.unobserve(entry.target);
        }
      });
    }, { threshold: 0.2 });

    document.querySelectorAll('.bench-group').forEach(group => {
      observer.observe(group);
    });
  </script>
</body>
</html>
